{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9bf11d07-9856-42da-8125-3099f10e0669",
   "metadata": {},
   "source": [
    "## Environment Setting\n",
    "Google drive mount (for Colab users) and package importing.\n",
    "You can optionally work on a transformer part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280812e1-0a56-4cf8-b6a7-7620874d9624",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Colab users\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=True)\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0,'/content/drive/{path to project directory}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b82d731-fc8f-4681-8e8b-614b58e21bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import json\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from data_utils import MLDataset, collate_fn\n",
    "from modeling import Seq2SeqModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78120bd-5408-45b6-9ea1-ec1f080349ca",
   "metadata": {},
   "source": [
    "## (Optional) Sample Visualization\n",
    "You can see actual sample images and correct answers. Additional matplotlib package is needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55336728-8e6c-4a3c-bf3d-b06e676b70ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just for reference: see actual samples\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "id_to_char = {}\n",
    "alphabets = \"abcdefghijklmnopqrstuvwxyz\"\n",
    "for i, c in enumerate(alphabets):\n",
    "    id_to_char[i+1] = c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6f4666f-5a79-47aa-94f7-00c85f938460",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: hear ([8, 5, 1, 18])\n",
      "Input image sequence:\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAxsAAAC8CAYAAAAQL7MCAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy81sbWrAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAYEklEQVR4nO3dfXBV9Z3H8ZPcRyAhIeSBEAhBHpWHoKsirgha0CKtAn1Yiq1rt2NdbTu12+0f29Wp4z/O1P7RDh3bbUd36zLWapEW1LVgC1Wq0NqCCRggQaJIQh4MeYTc5/1jpzNt9/MNHLw/cpO8X39+OJx77uX8zj3fnMmHvEwmk/EAAAAAIMvyh/sAAAAAAIxODBsAAAAAnGDYAAAAAOAEwwYAAAAAJxg2AAAAADjBsAEAAADACYYNAAAAAE4wbAAAAABwgmEDAAAAgBPBC90wLy/P5XEAH1omkxnuQ2CdIOexToDzy4V14nmsFeS+C1krPNkAAAAA4ATDBgAAAAAnGDYAAAAAOMGwAQAAAMCJC/4F8bEmP1//Upb1y1qpVNrl4QAAAAAjDk82AAAAADjBsAEAAADACYYNAAAAAE4wbAAAAABwgmEDAAAAgBNjvo0qGg3LfPXqa2V+2WVVMv/hD5+XeSyWuLgDAwAAQE7K89laakmPgTZTnmwAAAAAcIJhAwAAAIATDBsAAAAAnGDYAAAAAOAEwwYAAAAAJ8ZMG1UwGJD5ffd9QuaPPHKvzMNh/ZG99NLrMm9sPHkBRwdguAQCek0HQyGZl5ZV6O2DevtkUjfSdbSflnk8NihzAMAwMMqlikoKZD6+ICrzTEbvp6OlS+bJROq8hzZS8GQDAAAAgBMMGwAAAACcYNgAAAAA4ATDBgAAAAAnGDYAAAAAODFm2qgsEydOkHkkoptlAOQKXRFSXDJZ5lOrqmV+7fUrZV4+Zaqv7ScUFMq8v69X5rt37pD5L597SuanTjbLHADgTshoIa29bq7Mp8+aInOrXeqlZ/bKvK97QOaZtFFrlcN4sgEAAADACYYNAAAAAE4wbAAAAABwgmEDAAAAgBMMGwAAAACcGPNtVAByV3FJqflnVdNrZH7n578k86uvWy7zsvJKmQdDupEuEAjIPJXSTSN5ebo1a+bseb5ed8sTm2Xe1dkuc+Q+61wKGedAQUGBzK1zzPM8LxjUX/PRaNTX9pZEIiHztrY2mSeTSZlb6yedTvs6HmAooYh9fk8s1u2k1bP1d8SXH/mMzCur9fdWe8sZmf9u50GZD/SelXmKNioAAAAA+D8MGwAAAACcYNgAAAAA4ATDBgAAAAAnGDYAAAAAODFm2qhCIf1WCwvHZ2X/BQXjsrIfYCyKRPX6+cZD3zb/zjXLbpR5+ZQqmYfDEV/HFI8Nyrzp2Nsy37PrBZn/3bU36Hypzj995z0yt1qnnv7Px2WeTut2H1x6VuvU5MmTZV5eXi7zOXPmyHyoNiqrwaqsrMzX9lYr1MDAgMz37Nkj8/7+fpl3dXXJvK+vT+axWEzmgOd5nmcsifKpJeZfmbt4hswXXj3b174CQb3eY+fiMk8lR/+1micbAAAAAJxg2AAAAADgBMMGAAAAACcYNgAAAAA4wbABAAAAwIkx00Y1YUJU5tXVU2Sen6/nMKuRA8Bf0lUgJaW6AefGm9fI/LY7/sF8BavBKpVKyvzMBx0yb205KfPdO3fIfNdL22Qej+t2nPlX1MrcUjRJN5wUFet8qCYiuBGJ6GazoqIimS9fvlzmGzdulLnVOlVZWXkBR/fXQqGQzKNR/Z0YDOrbgkwmI/NkUq+3trY2mcfjupHn9ddfl/krr7wi861bt8qclqoxxrj8RaJhmdcum2fu6vrV+lo9f8lMmYejem11tffIvOnwezI/26+bD9NpveZGIp5sAAAAAHCCYQMAAACAEwwbAAAAAJxg2AAAAADgBMMGAAAAACfGTBtVT0+/zBsaTsg8nV7ha//9/ed8HxMw8ukqkOkzLpP5fV97UOZWG5XVOOV5nnemq1PmLzz/tMzf3PeqzBsOHZR5e1uLzONG2839//KQzJf+/U0yz8vTP+uJx/S1pL+vV+b48KzGJqtdau3atTJfuXKlzNes0ef35MmTZR4IBGSei6zPrqamxtd+qqqqZL5w4UKZW+1Vzc3Nvl4XI4TROhWO6POvfOokmS9fc5X5EouX6ha4skq9rw/adOtU3f5Gme/7db3MB/r0NT9DGxUAAAAADI1hAwAAAIATDBsAAAAAnGDYAAAAAOAEwwYAAAAAJ8ZMG1XG+KX+9Cj6bX/AFasVavVtG2T+lW88LHOrpcqqGjn57nHzmDZ/+1sy/9WLW2WeiOsWKb+mVev3sOnu+2UeHTde5oPnzsr8yR98R+Y/++//kHkqlZQ5LpzVOlVbWyvzdevWyXzRokUyz1brVDI5cv6t8/P1zzKtPBwOy3zixIm+tsfoVFJmrNFlc2X+kXVLZb56w3Xma4SjutkqnUrL/De//L3MX/rpazJ/p+F9mSdiI2ddXyyebAAAAABwgmEDAAAAgBMMGwAAAACcYNgAAAAA4ATDBgAAAAAnxkwbFYDzc986pXV1tsvcapzyPM/b9T/bZJ6t1imrReqOT31O5iWlZTKPG8djHf8vnntK5lZ7Ff6/vDzdbhaNRmW+du1amVutU9b2waC/r9RYTJ8bra2tMt+2TZ8z/f395muk07pJJ1usRq3ly5fL3GrsKi4ulvmkSZNkvnDhQpk3NTXJ3PXngCzRS9crq9TnwfwlM33lVuOU59nXDcvgWb1+B8/FZW61nwaC+uf+1vaZEdiiypMNAAAAAE4wbAAAAABwgmEDAAAAgBMMGwAAAACcYNgAAAAA4MSYaaMKhfRbLSzUjTPAaFZcUirzlat0y859X3tQ5lbrVDKZlPnxY2/L/Cc/+q7Mf/XiVpl7XvZap0LhiMxXrVkv8zs+dZexJ91kcuL4UZk/+fh3ZH7qvWZj/7hQkYj+N506darMV69eLfPFixfL3G/rlMVqkTp8+LDMn3vuOZn39fWZr5FKpWRuNe/4beSxPouSkhKZ19TUyNxqnSooKPC1n/x8q9mHNqqcYpxmkWhY5vOX1Bi5bp2y2qv8nt9D/Z2CIn3/WD5Vn/sZ4xy02qvO9g/KfKDvnMwTMf29mwt4sgEAAADACYYNAAAAAE4wbAAAAABwgmEDAAAAgBMMGwAAAACcGDNtVBUVuh1gxYqrZB4MBmSeTOpmj3HjdPtJaWmRr/14nuf19Z2VeSpFmwb8yc/X5/HGz90r8zu/8GWZTy4tl7nV8GK1Tj3yb3r/h956U+bZapwaSsWUKpnfdc9XZT6tukbmg+f0ut314vMyt1qqPC9j5PhbgYA+v2+99VaZr1u3Tubr1+vmsWg06ut4rNaaTMbfv6nVqHT55ZfL3Grf8jzPGxzUjTalpbqRbsKECec5ur9m/RssWLBA5oWFhb72b11j/H6mGB7haEjmFVX6nmzxdfNkfu+Dn5J5ubGfcES/7sXID+j1uHbTcpmv+NjVMk8mdFtUf4/+7mg6fFLmf9hzSOa/ffGPvvafTNj3odnGkw0AAAAATjBsAAAAAHCCYQMAAACAEwwbAAAAAJxg2AAAAADgxJhpowqF9FudONFf84bVEvLAAxtlXlAwXuYDA+fM13jhhb0y3779VZknLmGjAHKT1Tp17fUrZf6JTV+Q+eTSCpmnUrpF4w9v/FbmW574vsyHs3UqHNHNQtcsu1HmVdNrjD3pxqHXX90l8+0/3yLzeEy3BOHChUK6caa2tlbmS5YskbnVOmW1S1n8NiQVFBTI3Gpyspqfxo/X3zOe53nnzunvGquNyjomv4qKdBOjdazWcba2tsq8qalJ5lZ7FYZHWeUkmS9aOlfmN3z0SpmXT/XXOmWt3aGWtN+Cs8Jiff9YMNFej0o8lpB5UYlubiso0vs/1dwu82P178m8r3tA5pl09pveeLIBAAAAwAmGDQAAAABOMGwAAAAAcIJhAwAAAIATDBsAAAAAnBgzbVRW65TVUmXJz9dVBnff/TGZp43f6h+qMWPTpltl/sUvPirzZ57ZKfOY0XCAkSsY1M0bs+ZeIfPPfuHLMrealuJGK9Te3S/L3GqdslqqrFarbLKauW5YqdfVvV/9pswnlUyWeTqt29+s1qmW99+VOT68uXN1o82GDRtkPmfOHJknEvpaaTUkRSIRmVvtWFaLlLWf6upqmU+bNk3mQ7VmpVL+2gqtfXV2dsq8ra1N5rt375a51fx1/PhxmdfV1cl81y7d/kYb1fDID+ifXc9dPEPm16/WjXGLl+o1Go76a52y+G2cGorVTmr9GN86NyPjwjIvnVIs87z8mTKvqimX+XtNp2Xe33NW5hmPNioAAAAAIwTDBgAAAAAnGDYAAAAAOMGwAQAAAMAJhg0AAAAATjBsAAAAAHBizFTfrlhxlcwrKkqysv9USleavfrqAZk3NJww97Vx4y0yf+CBjTI/cOCozOvqmszXQK7TdX5Wxe1Dj+oK2oW1V8vcqrg99NabMt/82MMybzxySOZWPWx26c9oWrWuBdyw8W5f21v7t6ps39z/mswvzWcxulkVk7Nnz5b5lClTZB4O64rJU6dOyfzECX2dnjxZ1yKXl+vqyUmTJsncb22nJTNEn2dPT4/MBwcHfR3Tvn37ZH7okL4G+K2+bWlpkXl7e7vMYzF9DYNjxikbjujbyflL9PXVysun6nsyv2vFqpnNGP8dwcWw1p11Pxgf1BXbecZ/qTB4Vp/jPV39Mu/q6JV5bDAu86GuG9nGkw0AAAAATjBsAAAAAHCCYQMAAACAEwwbAAAAAJxg2AAAAADgxJhpoyosHC/zUCg7H8GJE7pJY/PmZ2Xe2Pieua9Vq66V+dy51b62f/tt3aSSTNKOk+uqqmtk/k/3/6vMrdapcDgi89/s3C7zzd/+lsyPH3tb5sPZtFRSWibzf37g32V+w8pbZZ6fH5B5PKbberb/fIvMuzo7ZI4Pz2qjWrBggcyt9ierfWXbtm0y37p1q8z7+vpkPmvWLJlbx2m9L7+s5h3P87yGhgaZWy1PVmNXXV2dzHt7jQYcn21Rl7IZB+cXGafPg/Kpem0tvm6ezO+4a6XMy4zWqXAkJHOrUamz9YzMGw/pe6wjB+0m0LTPpiqr2eqc0SLV3anXSsh4z13tuknu5PHTMm8+pu9DE/GkzL1LuOR4sgEAAADACYYNAAAAAE4wbAAAAABwgmEDAAAAgBMMGwAAAACcGDNtVH7l5eXJPJHQv9W/adNDMv/jH3UTSDis2wc8z/MeffQnMv/xj78p8xUrrpT5L36xR+bvvKMbC3DphSNRmd/xyc/JfPWa9Xo/RuvUma5OmW99+kmZNx45JPPhap2KRMeZf3bjzWt85dZnbVVynDh+VOY7X3ze137gTrbanCyJRELmjY2NMm9p0dfW+vr6rB2TX93d3TIfHNRta4GAbmezGrhSKdoNRzR9q+O7deqGj+r7kNJKvZ9QWN9+Wq1k7ae6ZF63/5jM33hFt6e99Ya+rnue/zYqS8po/IzH9PXEuo5ZDVxn+/XaTcSM1qkcwJMNAAAAAE4wbAAAAABwgmEDAAAAgBMMGwAAAACcYNgAAAAA4ARtVFnS09Mvc6NYwYsZrQSe53mvvXZA5lYT1s03XyPz9etXyvx73/uZzJNGgwLcqblsrsxXr90g8+i48TJPpfS58cLzT8t8755fyXy4Wqfy83UDzurb9OfgeZ5339celHlJaZmv1z7T9YHMf/Kj78rcaqmCO1ZDTVeXbqg5c+aMzIuLi2V+++23y7ysTJ9Ljz32mMxPnz4t8+bmZplb7yudTvvKgfMyWqfCEd2M6bd1qvY6/V1m7d8SH9T3Rm/t061Te1/W90tWS1Xrux3ma1vrMVv87t46nkyWWrMuJZ5sAAAAAHCCYQMAAACAEwwbAAAAAJxg2AAAAADgBMMGAAAAACfGTBtVfr5RxZCDWlt1O05bm25eqa6eIvNlyxbJ/KmnXpJ5R0f3+Q8OFyUQ0EvtIx+9Q+az5lzua/8fdLTJ/Dc7t8t8QkGhsSed9/f2+DqewqJimU+dNkPmd37+SzK/bd1G8zXC4YjM8/L0Wrcau37/+h6Z//bXep0k4jHzmOCG1cpy+PBhmR86dEjmtbW1Mp8yxbqGLpP5+vXrfb1ufX29zGMxfS4NDAzIvLu7W+ZDtegkk/q8x+iUZ9zrFJUUyHzJMt06df+3Pi3ziqrJMg9HdetU3GjebD+l72es1qnHH35G76dFN8/FBuMy90ZekdOowJMNAAAAAE4wbAAAAABwgmEDAAAAgBMMGwAAAACcYNgAAAAA4MSoa6OyWqcWLLjM2F7PW1a7x1CtH9mSSOj2ECu33vOSJXNlPn16hcxpo7r0zg70yzwR100awaBu/CguKZX5P97zgMxjscHzH9xfeKexwdf2s+ctkPnlC5fIvLxiqsxDIf1+h2Kt0Z5u3Vqy88Wtevszui0Fl146nZb53r17Zb59u25hs/ZjtVRVVOhr5de//nWZWy1SPT26za23t1fmzc3NMj9w4IDMu7rsc/XZZ5+VudVsZX1GGBms+4HxBVGZV8+ulHlZ5SSZW61TVgtgZ6u+7tbt161Te1/W5zitUyMbTzYAAAAAOMGwAQAAAMAJhg0AAAAATjBsAAAAAHCCYQMAAACAE6OwjUrPT4sWzTa21w0KloaGZpmfPNnmaz9DsdpA6uubZD5r1jSZV1dPkflnPnOLzP/0p6MXcHS4GKmUbhL79cu/lPk1y26U+YpVa2UeDkeM7W+7gKM7P78NNVYzidUU1XrqPZkfP2a3YC27cZXMrc/CujbMv0I3Eb37TqPMT51slnlPN+1Vl1rcaG3bsmWLzHfs2CHzK6+8UuYzZ86U+bJly2Q+Y8YMmRcWFsp8woQJMq+urpb5sWO6wcdqlgL+LBAMyHzcBH29DIb07aF1bU+n9HfE0bp3Zf7GrrdkbrVU0To1svFkAwAAAIATDBsAAAAAnGDYAAAAAOAEwwYAAAAAJxg2AAAAADgx6tqosmXQaD7Ytm2Pr+0vRjqt6xXq64/L/OMfXy7zkNEmcdNNV8s8aLRVJJMpmePDa3lfN3VseeL7Mk8kEjKfd8VimRcVTZL5xGKdW21RfT3dMu/t1flAf5/M9/9ut8zf3PeqzE+3vC9zz/O8isoqmc+9fJHMi4z3fNc9X5X5qtvWy3zLE5tl/tP/elzmuPSsdqbe3l6Z9/f3y/zgwYMyP3HihMxrampkbrVRWS1vAwMDMt+zZ4/Mz5w5I3PP87y+Pr0W/TbMYWRLGd/j5wZiMo/H9HeNJRHXjYtHDuq1cuRgs8w7WoxWP1qnRjSebAAAAABwgmEDAAAAgBMMGwAAAACcYNgAAAAA4ATDBgAAAAAnxkwbldWoZLXvvP9+u8x37Hgta8fkV319k8y7u3WTSllZscynT6+QeXFxgcw7O3vOf3C4KOm0Pi+t1qajDXUyt1qn5sxfKHOrsclqqGk6eljmjUaeiOuGk86ONpnHY3r7YCgkc8/zvM2PPSzz2z/5WZlb79ly9G39WR9vbPC1H1x61nls5R0dHb7y5uZmmefnu/35XTKpG3+AP0sljXO8VTeWvbJtn6/9j5sQkbnVamXt3zqeRIxzfDTiyQYAAAAAJxg2AAAAADjBsAEAAADACYYNAAAAAE4wbAAAAABwIi9j1TH97YZ5ea6PxalFi2bJfOlS3dazf/8hmR850izzREK3CmVTTU2lzJ988iGZX3XVPJnX1elWq1tu+YrMBwfjF3B0w+8CT2Wncm2d5OXpnyf4bc2xWnwyGZ1fCvn5AZkXTSqR+cSJxb7239vbLfPurg9kPpyfhR+sE+D8cmGdeF4W14qxm1BYl5KWVeqGw0BQX3dTRuOn2ToVN1qncuNjhw8XslZ4sgEAAADACYYNAAAAAE4wbAAAAABwgmEDAAAAgBMMGwAAAACcGDNtVJZAQM9bqVTuNcuEQroFYv78Gpn7bdqqrz9+UceVK3KhPWS0rhOMHqwT4PxyYZ143vCtlUAwOz+LTiVz714K2UUbFQAAAIBhw7ABAAAAwAmGDQAAAABOMGwAAAAAcIJhAwAAAIATY76NajQbSU1b2ZAL7SGsE+Q61glwfrmwTjyPtYLcRxsVAAAAgGHDsAEAAADACYYNAAAAAE4wbAAAAABwgmEDAAAAgBPB4T4AuDNaW6cAAAAwMvBkAwAAAIATDBsAAAAAnGDYAAAAAOAEwwYAAAAAJxg2AAAAADiRl8lkMsN9EAAAAABGH55sAAAAAHCCYQMAAACAEwwbAAAAAJxg2AAAAADgBMMGAAAAACcYNgAAAAA4wbABAAAAwAmGDQAAAABOMGwAAAAAcOJ/AYLNYsR4r/FSAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 1000x400 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Just for reference: see actual samples\n",
    "idx = 1234\n",
    "sample = np.load(f'./data_final/imgs/train/{idx}.npy')\n",
    "with open('./data_final/labels/train.json', 'r') as f:\n",
    "    sample_target = json.load(f)[str(idx)]\n",
    "    \n",
    "tgt_char = \"\"\n",
    "for i in sample_target:\n",
    "    tgt_char += id_to_char[i]\n",
    "\n",
    "\n",
    "print(f\"Answer: {tgt_char} ({sample_target})\")\n",
    "print(\"Input image sequence:\")\n",
    "\n",
    "plt.figure(figsize=(10, len(sample)))\n",
    "for i, img in enumerate(sample):    \n",
    "    plt.subplot(1, len(sample), i+1)\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400da0bd-7c64-4d23-a24b-b0d097d995ae",
   "metadata": {},
   "source": [
    "## Device and seed setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4579ad2-6031-450c-a30c-cbf5b69fbfac",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'torch._C' has no attribute '_cuda_setDevice'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#assert torch.cuda.is_available()\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Use 0th GPU for training\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mset_device(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m      5\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda:0\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# fix random seed to increase reproducibility\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# NOTE: Do not modify here!\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/torch_tutorial/lib/python3.11/site-packages/torch/cuda/__init__.py:350\u001b[0m, in \u001b[0;36mset_device\u001b[0;34m(device)\u001b[0m\n\u001b[1;32m    348\u001b[0m device \u001b[38;5;241m=\u001b[39m _get_device_index(device)\n\u001b[1;32m    349\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m device \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 350\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_cuda_setDevice(device)\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'torch._C' has no attribute '_cuda_setDevice'"
     ]
    }
   ],
   "source": [
    "#assert torch.cuda.is_available()\n",
    "\n",
    "# Use 0th GPU for training\n",
    "torch.cuda.set_device(0)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# fix random seed to increase reproducibility\n",
    "# NOTE: Do not modify here!\n",
    "NUM_CLASSES = 26 + 2 # 26 alphabets + 1 padding index + 1 <s> token (start token)\n",
    "\n",
    "random_seed = 7\n",
    "torch.manual_seed(random_seed)\n",
    "os.environ['PYTHONHASHSEED'] = str(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "random.seed(random_seed)\n",
    "torch.cuda.manual_seed(random_seed)\n",
    "\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "# %env CUBLAS_WORKSPACE_CONFIG=:16:8\n",
    "\n",
    "def seed_worker(worker_seed):\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "\n",
    "num_workers = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc6523c-962c-42fb-bbaa-38d8236797eb",
   "metadata": {},
   "source": [
    "## Model loading and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ffaa119-e2d5-4f4e-a663-5578c4f49359",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: modify path and batch size for your setting\n",
    "# NOTE: you can apply custom preprocessing to the training data\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "train_ds = MLDataset('data_final/imgs/train', 'data_final/labels/train.json')\n",
    "valid_ds = MLDataset('data_final/imgs/valid_normal', 'data_final/labels/valid_normal.json')\n",
    "\n",
    "train_dl = DataLoader(train_ds, batch_size=BATCH_SIZE, collate_fn=collate_fn, shuffle=True)\n",
    "valid_dl = DataLoader(valid_ds, batch_size=BATCH_SIZE, collate_fn=collate_fn, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "77af1ae4-496f-4cbd-a2dc-9c429d052d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can add or modify your Seq2SeqModel's hyperparameter (keys and values)\n",
    "kwargs = {\n",
    "    'hidden_dim': 64,\n",
    "    'n_rnn_layers': 2,\n",
    "    'rnn_dropout': 0.3,\n",
    "    'device': device\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "595dcc8b-006e-4730-94f6-933c2c743996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seq2SeqModel(\n",
      "  (encoder): Encoder(\n",
      "    (cnn): CustomCNN(\n",
      "      (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (fc1): Linear(in_features=1152, out_features=64, bias=True)\n",
      "    )\n",
      "    (rnn): LSTM(64, 64, num_layers=2, batch_first=True)\n",
      "    (fc): Linear(in_features=64, out_features=64, bias=True)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (embedding): Embedding(28, 64, padding_idx=0)\n",
      "    (rnn): LSTM(64, 64, num_layers=2, batch_first=True, dropout=0.5)\n",
      "    (lm_head): Linear(in_features=64, out_features=28, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "NUM_CLASSES =26+2\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = Seq2SeqModel(num_classes=NUM_CLASSES, **kwargs).to(device)\n",
    "print(model)\n",
    "##############################################################################\n",
    "#                          IMPLEMENT YOUR CODE                               #\n",
    "\n",
    "# Define the optimizer and loss function\n",
    "model_optim = optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=0)  # Assuming padding index is 0\n",
    "\n",
    "# You can define additional components like lr_scheduler, ...\n",
    "lr_scheduler = optim.lr_scheduler.StepLR(model_optim, step_size=10, gamma=0.1)\n",
    "# NOTE: you can define additional components like lr_scheduler, ...\n",
    "##############################################################################\n",
    "#                          END OF YOUR CODE                                  #\n",
    "##############################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1cc52297-5653-4a17-8ec1-b529bb0e9c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: you can freely modify or add training hyperparameters\n",
    "print_interval = 1000\n",
    "max_epoch = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cd18ddf1-a8d0-4ef6-a05f-ac4a38678b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, model_optim, loss_fn, max_epoch, train_dl, valid_dl, load_path=None, save_path='./model.pt'):\n",
    "    ##############################################################################\n",
    "    #                          IMPLEMENT YOUR CODE                               #\n",
    "    ##############################################################################\n",
    "    # Load your states\n",
    "    loaded_epoch = 0\n",
    "    loaded_best_valid_loss = -1\n",
    "    if load_path is not None:\n",
    "        state = torch.load(load_path)\n",
    "        model.load_state_dict(state[\"model\"])\n",
    "        model_optim.load_state_dict(state[\"optimizer\"])\n",
    "        loaded_epoch = state[\"epoch\"]\n",
    "        loaded_best_valid_loss = state[\"best_valid_loss\"]\n",
    "        # ...\n",
    "        \n",
    "    ##############################################################################\n",
    "    #                          END OF YOUR CODE                                  #\n",
    "    ##############################################################################\n",
    "    best_valid_loss = 1e+10 if loaded_best_valid_loss == -1 else loaded_best_valid_loss\n",
    "\n",
    "    for epoch in np.array(list(range(max_epoch - loaded_epoch))) + loaded_epoch:\n",
    "        step = 0\n",
    "        train_loss = 0\n",
    "        model.train()\n",
    "        for batch_idx, (data, target, lengths) in enumerate(tqdm(train_dl)):\n",
    "            data = data.to(device) # (B, T, H, W, C)\n",
    "            target = target.to(device) # (B, T)\n",
    "            #print(target[0, :])\n",
    "            \n",
    "            # start tokens should be located at the first position of the decoder input\n",
    "            start_tokens = (torch.ones([target.size(0), 1]) * 27).to(torch.long).to(device)\n",
    "            # print(start_tokens[0])\n",
    "            ##############################################################################\n",
    "            #                          IMPLEMENT YOUR CODE                               #\n",
    "            ##############################################################################\n",
    "            # Problem 5: implement loss calculation and optimization part\n",
    "            # You can utilize teacher-forcing strategy to this part\n",
    "            model_optim.zero_grad()\n",
    "            decoder_input = torch.cat([start_tokens, target[:, :-1]], dim=1)  # Teacher-forcing\n",
    "            #print(decoder_input[0, :])\n",
    "            logits, _ = model(data, lengths, decoder_input)\n",
    "            #print(logits.shape)\n",
    "            loss = loss_fn(logits.view(-1, logits.size(-1)), target.view(-1))\n",
    "            loss.backward()\n",
    "            model_optim.step()\n",
    "            ##############################################################################\n",
    "            #                          END OF YOUR CODE                                  #\n",
    "            ##############################################################################\n",
    "            train_loss += loss.detach().cpu().item()\n",
    "            step += 1\n",
    "            if (batch_idx + 1) % print_interval == 0:\n",
    "                print('epoch:', epoch + 1, 'step:', step + 1, 'loss:', loss.detach().cpu().item())\n",
    "                \n",
    "        train_loss_avg = train_loss / (batch_idx+1)\n",
    "        print(f\"epoch {epoch + 1}, train loss: {train_loss_avg}\")\n",
    "\n",
    "        valid_loss = 0\n",
    "        model.eval()\n",
    "        for batch_idx, (data, target, lengths) in enumerate(tqdm(valid_dl)):            \n",
    "            with torch.no_grad():\n",
    "                data = data.to(device) # (B, T, H, W, C)\n",
    "                target = target.to(device) # (B, T)\n",
    "                \n",
    "                # start tokens should be located at the first position of the decoder input\n",
    "                start_tokens = (torch.ones([target.size(0), 1]) * 27).to(torch.long).to(device)\n",
    "                ##############################################################################\n",
    "                #                          IMPLEMENT YOUR CODE                               #\n",
    "                ##############################################################################\n",
    "                # Implement loss calculation for valid batch (same as problem 5)\n",
    "                decoder_input = torch.cat([start_tokens, target[:, :-1]], dim=1)  # Teacher-forcing\n",
    "                logits, _ = model(data, lengths, decoder_input)\n",
    "                loss = loss_fn(logits.view(-1, logits.size(-1)), target.view(-1))\n",
    "                ##############################################################################\n",
    "                #                          END OF YOUR CODE                                  #\n",
    "                ##############################################################################\n",
    "                valid_loss += loss.cpu().item()\n",
    "        valid_loss /= (batch_idx + 1)      \n",
    "        if valid_loss < best_valid_loss:\n",
    "            print(\"New best valid loss, saving model\")\n",
    "            ##############################################################################\n",
    "            #                          IMPLEMENT YOUR CODE                               #\n",
    "            ##############################################################################\n",
    "            # Save your states\n",
    "            state = {\n",
    "                \"model\": model.state_dict(),\n",
    "                \"optimizer\": model_optim.state_dict(),\n",
    "                \"epoch\": epoch + 1,\n",
    "                \"best_valid_loss\": best_valid_loss,\n",
    "                # ...\n",
    "            }\n",
    "            ##############################################################################\n",
    "            #                          END OF YOUR CODE                                  #\n",
    "            ##############################################################################\n",
    "            torch.save(state, save_path)\n",
    "            best_valid_loss = valid_loss\n",
    "        print('valid epoch: %d, valid loss: %.4f, best valid loss: %.4f' % (epoch + 1, valid_loss, best_valid_loss))\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fdb670ac-6d52-458d-b9e8-ded745258c7b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|████▎                                                                             | 28/531 [00:12<03:46,  2.22it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m load_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m train(model, model_optim, loss_fn, max_epoch, train_dl, valid_dl, load_path\u001b[38;5;241m=\u001b[39mload_path, save_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./model.pt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[13], line 25\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, model_optim, loss_fn, max_epoch, train_dl, valid_dl, load_path, save_path)\u001b[0m\n\u001b[1;32m     23\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     24\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m---> 25\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, (data, target, lengths) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(tqdm(train_dl)):\n\u001b[1;32m     26\u001b[0m     data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;66;03m# (B, T, H, W, C)\u001b[39;00m\n\u001b[1;32m     27\u001b[0m     target \u001b[38;5;241m=\u001b[39m target\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;66;03m# (B, T)\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/torch_tutorial/lib/python3.11/site-packages/tqdm/std.py:1178\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1175\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1177\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1178\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1179\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1180\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1181\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/torch_tutorial/lib/python3.11/site-packages/torch/utils/data/dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    631\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    632\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 633\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[1;32m    634\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    636\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/anaconda3/envs/torch_tutorial/lib/python3.11/site-packages/torch/utils/data/dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    676\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 677\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_fetcher\u001b[38;5;241m.\u001b[39mfetch(index)  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    678\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    679\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/anaconda3/envs/torch_tutorial/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/anaconda3/envs/torch_tutorial/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/ml_final/data_utils.py:21\u001b[0m, in \u001b[0;36mMLDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     18\u001b[0m seq_length \u001b[38;5;241m=\u001b[39m imgs\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     19\u001b[0m label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabels[\u001b[38;5;28mstr\u001b[39m(idx)]\n\u001b[0;32m---> 21\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mtensor(imgs, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32), torch\u001b[38;5;241m.\u001b[39mtensor(label, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong), seq_length\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "load_path = None\n",
    "train(model, model_optim, loss_fn, max_epoch, train_dl, valid_dl, load_path=load_path, save_path='./model.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e83246b-de55-4668-bb78-c1fbb6c7f60d",
   "metadata": {},
   "source": [
    "## Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c3f3646f-f3a3-4d7f-b71c-9665f11450d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs_generate = {\n",
    "    # you can add arguments for your model's generate function\n",
    "    'max_length': 10\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0ce38064-e195-46c5-851c-43b116be5333",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not modify this cell!\n",
    "\n",
    "def eval(dataloader, model_path):\n",
    "    state = torch.load(model_path)\n",
    "    model.load_state_dict(state[\"model\"])\n",
    "    model.eval()\n",
    "\n",
    "    id_to_char = {}\n",
    "    id_to_char[0] = \"<pad>\"\n",
    "    id_to_char[27] = \"<s>\"\n",
    "    alphabets = \"abcdefghijklmnopqrstuvwxyz\"\n",
    "    for i, c in enumerate(alphabets):\n",
    "        id_to_char[i+1] = c\n",
    "\n",
    "    results = []\n",
    "    labels = []    \n",
    "    for batch_idx, (data, target, lengths) in enumerate(tqdm(dataloader)):       \n",
    "        data = data.to(device) # (B, T, H, W, C)\n",
    "        target = target.to(device) # (B, T)\n",
    "        \n",
    "        # start tokens should be located at the first position of the decoder input\n",
    "        start_tokens = (torch.ones([target.size(0), 1]) * 27).to(torch.long).to(device)\n",
    "        with torch.no_grad():\n",
    "            generated_tok = model.generate(data, lengths, start_tokens, **kwargs_generate) # (B, T)\n",
    "            \n",
    "        for i in range(generated_tok.size(0)):\n",
    "            decoded = \"\"\n",
    "            for j in generated_tok[i][:lengths[i].int()].tolist():\n",
    "                decoded += id_to_char[j]\n",
    "            results.append(decoded)\n",
    "    \n",
    "            decoded = \"\"\n",
    "            for j in target[i][:lengths[i].int()].tolist():\n",
    "                decoded += id_to_char[j]\n",
    "            labels.append(decoded)\n",
    "        \n",
    "    corrects = []\n",
    "    for i in range(len(results)):\n",
    "        if results[i] == labels[i]:\n",
    "            corrects.append(1)\n",
    "        else:\n",
    "            corrects.append(0)\n",
    "    print(\"Accuracy: %.5f\" % (sum(corrects) / len(corrects)))\n",
    "\n",
    "    return results, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8ce11344-4b75-41a7-9701-9d372f840f76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation with validation set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 76/76 [00:10<00:00,  7.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.00000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# load and evaluate your model\n",
    "load_path = './model.pt'\n",
    "print(\"Evaluation with validation set\")\n",
    "results, labels = eval(valid_dl, load_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2c96d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:torch_tutorial] *",
   "language": "python",
   "name": "conda-env-torch_tutorial-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
